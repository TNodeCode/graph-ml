{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f1139c3b-8db4-4f74-8293-088ef1eb5297",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.nn import SGConv\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994c777a-1e34-4db8-87be-a91ec92cb510",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "First we load the predefined Planetoid dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f5fee13f-f5a0-4af2-8a2d-0903b9147aad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cora Data(x=[2708, 1433], edge_index=[2, 10556], y=[2708], train_mask=[2708], val_mask=[2708], test_mask=[2708])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "path = './data'\n",
    "dataset = Planetoid(path, \"Cora\")\n",
    "data = dataset[0]\n",
    "print('Cora', data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "230d2d75-53c3-4b7f-ad6a-35cf2e1d869a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device cpu\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('Device', device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8b23719-c17d-4cd4-9a79-615cf57dd5c4",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "Let's construct our graph neural network, whcih will consist of a single SGConv layer only.\n",
    "\n",
    "SGConv: https://pytorch-geometric.readthedocs.io/en/latest/_modules/torch_geometric/nn/conv/sg_conv.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "17591eaa-a5dc-4c66-9dc0-13f5f6816f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In Channels: 1433\n",
      "Out Channels: 7\n"
     ]
    }
   ],
   "source": [
    "# Simple Graph Convolutional Neural Network\n",
    "model = SGConv(\n",
    "    in_channels=data.num_features,    # dimension of the input vectors\n",
    "    out_channels=dataset.num_classes, # dimension of the output vectors\n",
    "    K=1,                              # size of neighborhood for each node (1 means only look at direct neighbors)\n",
    "    cached=True                       # cache can save time, but required more memory\n",
    ")\n",
    "\n",
    "print(\"In Channels:\", data.num_features)\n",
    "print(\"Out Channels:\", dataset.num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7363abd-a2df-4c29-b8bc-78707ca847dc",
   "metadata": {},
   "source": [
    "### Compute embeddings for nodes\n",
    "\n",
    "Before we train the model let's just pass some data through it to see how the embeddings look like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40a57b80-81ee-4ecf-ad28-ec303d078101",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the original data: torch.Size([2708, 1433])\n",
      "Shape of the embedded data: torch.Size([2708, 7])\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the original data:\", data.x.shape)\n",
    "print(\"Shape of the embedded data:\", model(data.x, data.edge_index).shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b65abbf9-5364-40c9-a53d-59bf61d55ea2",
   "metadata": {},
   "source": [
    "The input vectors have 1433 components whereas the output vectors only have seven, which corresponds to the number of classes in the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f100fbd-20fe-41e3-9d33-bd5419304277",
   "metadata": {},
   "source": [
    "### More complex model\n",
    "\n",
    "Now let's build a more complex model that consists of multiple layers. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "75b3057c-86b4-4c96-9f3e-b9a998d795c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGNet(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim, K=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = SGConv(\n",
    "            in_channels=input_dim,\n",
    "            out_channels=output_dim,\n",
    "            K=K,\n",
    "            cached=True\n",
    "        )\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "        \n",
    "    def forward(self, data):\n",
    "        x = self.conv1(data.x, data.edge_index) # Pass the data through the SGConv layer\n",
    "        x = self.softmax(x)                     # Turns the output into a probability distrobution\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cc8843d5-4f07-4c3c-9322-29d8e37b5011",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model and the data and move them to the chosen device (CPU / CUDA)\n",
    "model, data = SGNet(data.num_features, dataset.num_classes).to(device), data.to(device)\n",
    "\n",
    "# Select an optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.2, weight_decay=0.005)\n",
    "\n",
    "#  Loss function\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "079c912d-1c55-464c-9d0d-151bd9993727",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter conv1.lin.weight:\n",
      "Shape: torch.Size([7, 1433])\n",
      "Parameter conv1.lin.bias:\n",
      "Shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "# Inspect the shape of te parameters\n",
    "for i, param in model.named_parameters():\n",
    "    print(\"Parameter {}:\".format(i))\n",
    "    print(\"Shape:\", param.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0d31d4c-5184-4867-ba10-a11b75e89226",
   "metadata": {},
   "source": [
    "The graph convolution layer has two parameter matrices:\n",
    "- The matrix that is multiplied with the feature vectors of the nodes\n",
    "- The bias that is added to the results of the convolution operation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4211d3-7ea1-4e34-a3cd-45481f09f7b2",
   "metadata": {},
   "source": [
    "## Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "034e440e-c451-4611-ae15-15d571eedc03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Reset the gradients\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward the data through the model and compute predictions.\n",
    "    predicted_classes = model(data)\n",
    "    \n",
    "    # Compute loss\n",
    "    loss = criterion(predicted_classes[data.train_mask], data.y[data.train_mask])\n",
    "    \n",
    "    # Backpropagation\n",
    "    loss.backward()\n",
    "    \n",
    "    # Update the parameters\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "6a12062d-634b-437b-ab20-b0d9ff4de71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # Set the model ot evaluation mode\n",
    "    model.eval()\n",
    "    \n",
    "    # Compute logits\n",
    "    logits = model(data)\n",
    "    \n",
    "    # Accuracies for training, validation and test data\n",
    "    accs = []\n",
    "    \n",
    "    # Compute accuracies for training, validation and test dataset\n",
    "    for _, mask in data('train_mask', 'val_mask', 'test_mask'):\n",
    "        # Get predicted class\n",
    "        pred = logits[mask].max(1)[1]\n",
    "        #  Compute the accuracy\n",
    "        acc = pred.eq(data.y[mask]).sum().item() / mask.sum().item()\n",
    "        # Save accuracy\n",
    "        accs.append(acc)\n",
    "        \n",
    "    return accs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "aff2670e-5cf5-4684-af60-79a3f26ab9d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, Train: 0.9500, Val: 0.4680, Test: 0.4910\n",
      "Epoch: 002, Train: 0.9357, Val: 0.7180, Test: 0.7150\n",
      "Epoch: 003, Train: 0.9643, Val: 0.7120, Test: 0.7150\n",
      "Epoch: 004, Train: 0.9643, Val: 0.6300, Test: 0.7150\n",
      "Epoch: 005, Train: 0.9500, Val: 0.6820, Test: 0.7150\n",
      "Epoch: 006, Train: 0.9714, Val: 0.7180, Test: 0.7150\n",
      "Epoch: 007, Train: 0.9714, Val: 0.7360, Test: 0.7850\n",
      "Epoch: 008, Train: 0.9714, Val: 0.7300, Test: 0.7850\n",
      "Epoch: 009, Train: 0.9714, Val: 0.7220, Test: 0.7850\n",
      "Epoch: 010, Train: 0.9714, Val: 0.7040, Test: 0.7850\n",
      "Epoch: 011, Train: 0.9643, Val: 0.7020, Test: 0.7850\n",
      "Epoch: 012, Train: 0.9643, Val: 0.7100, Test: 0.7850\n",
      "Epoch: 013, Train: 0.9500, Val: 0.7080, Test: 0.7850\n",
      "Epoch: 014, Train: 0.9571, Val: 0.7060, Test: 0.7850\n",
      "Epoch: 015, Train: 0.9571, Val: 0.7140, Test: 0.7850\n",
      "Epoch: 016, Train: 0.9643, Val: 0.7100, Test: 0.7850\n",
      "Epoch: 017, Train: 0.9643, Val: 0.7140, Test: 0.7850\n",
      "Epoch: 018, Train: 0.9786, Val: 0.7300, Test: 0.7850\n",
      "Epoch: 019, Train: 0.9786, Val: 0.7400, Test: 0.7820\n",
      "Epoch: 020, Train: 0.9786, Val: 0.7200, Test: 0.7820\n",
      "Epoch: 021, Train: 0.9857, Val: 0.7360, Test: 0.7820\n",
      "Epoch: 022, Train: 0.9786, Val: 0.7340, Test: 0.7820\n",
      "Epoch: 023, Train: 0.9714, Val: 0.7380, Test: 0.7820\n",
      "Epoch: 024, Train: 0.9714, Val: 0.7420, Test: 0.7780\n",
      "Epoch: 025, Train: 0.9643, Val: 0.7460, Test: 0.7740\n",
      "Epoch: 026, Train: 0.9643, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 027, Train: 0.9643, Val: 0.7360, Test: 0.7740\n",
      "Epoch: 028, Train: 0.9643, Val: 0.7300, Test: 0.7740\n",
      "Epoch: 029, Train: 0.9786, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 030, Train: 0.9714, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 031, Train: 0.9714, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 032, Train: 0.9714, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 033, Train: 0.9786, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 034, Train: 0.9857, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 035, Train: 0.9786, Val: 0.7360, Test: 0.7740\n",
      "Epoch: 036, Train: 0.9714, Val: 0.7300, Test: 0.7740\n",
      "Epoch: 037, Train: 0.9714, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 038, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 039, Train: 0.9643, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 040, Train: 0.9643, Val: 0.7340, Test: 0.7740\n",
      "Epoch: 041, Train: 0.9643, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 042, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 043, Train: 0.9643, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 044, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 045, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 046, Train: 0.9643, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 047, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 048, Train: 0.9643, Val: 0.7360, Test: 0.7740\n",
      "Epoch: 049, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 050, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 051, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 052, Train: 0.9714, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 053, Train: 0.9643, Val: 0.7360, Test: 0.7740\n",
      "Epoch: 054, Train: 0.9643, Val: 0.7360, Test: 0.7740\n",
      "Epoch: 055, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 056, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 057, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 058, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 059, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 060, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 061, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 062, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 063, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 064, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 065, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 066, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 067, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 068, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 069, Train: 0.9643, Val: 0.7320, Test: 0.7740\n",
      "Epoch: 070, Train: 0.9643, Val: 0.7380, Test: 0.7740\n",
      "Epoch: 071, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 072, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 073, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 074, Train: 0.9643, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 075, Train: 0.9643, Val: 0.7440, Test: 0.7740\n",
      "Epoch: 076, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 077, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 078, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 079, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 080, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 081, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 082, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 083, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 084, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 085, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 086, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 087, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 088, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 089, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 090, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 091, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 092, Train: 0.9643, Val: 0.7420, Test: 0.7740\n",
      "Epoch: 093, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 094, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 095, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 096, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 097, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 098, Train: 0.9643, Val: 0.7400, Test: 0.7740\n",
      "Epoch: 099, Train: 0.9643, Val: 0.7400, Test: 0.7740\n"
     ]
    }
   ],
   "source": [
    "best_val_acc = test_acc = 0\n",
    "\n",
    "for epoch in range(1, 100):\n",
    "    train()\n",
    "    train_acc, val_acc, tmp_test_acc = test()\n",
    "    if val_acc > best_val_acc:\n",
    "        best_val_acc = val_acc\n",
    "        test_acc = tmp_test_acc\n",
    "    msg = \"Epoch: {:03d}, Train: {:.4f}, Val: {:.4f}, Test: {:.4f}\"\n",
    "    print(msg.format(epoch, train_acc, val_acc, test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da877a79-5bb4-444c-be4f-1ae6ee97ff09",
   "metadata": {},
   "source": [
    "We can see a clear overfitting of teh model. The training set reaches an accuracy of 96%, wherease the validation set achieves 74% and the test set achieves 77% accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a230df5-df7e-4aa2-9c0d-a564dd483eeb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
